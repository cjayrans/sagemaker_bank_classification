{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group_name = \"FG-job-aggs-d34eb034\"\n",
    "output_name = \"45e79823-e575-4f06-9723-0e7fb6f1e6b2.default\"\n",
    "flow_uri='s3://sagemaker-us-east-1-769265885190/data_wrangler_flows/flow-01-01-10-10-33a59ad7.flow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker Python SDK version 2.x is required\n",
    "import sagemaker\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import uuid\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "from zipfile import ZipFile\n",
    "import inspect\n",
    "\n",
    "#module containing utility functions for this notebook\n",
    "import pipeline_utils\n",
    "\n",
    "original_version = sagemaker.__version__\n",
    "if sagemaker.__version__ != \"2.20.0\":\n",
    "    subprocess.check_call(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"sagemaker==2.20.0\"]\n",
    "    )\n",
    "    import importlib\n",
    "    importlib.reload(sagemaker)\n",
    "    \n",
    "# S3 bucket for saving processing job outputs\n",
    "# Feel free to specify a different bucket here if you wish.\n",
    "sess = sagemaker.Session()\n",
    "default_bucket = sagemaker.session.Session().default_bucket()\n",
    "sm_client = boto3.client('sagemaker')\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "region = sess.boto_region_name\n",
    "base_job_prefix=\"sagemaker/DEMO-xgboost-banking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-769265885190/offline-store/query_results/\n",
      "sagemaker_featurestore\n",
      "Running query:\n",
      " SELECT COUNT(*) FROM \"fg-job-aggs-d34eb034-1646097010\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_utils.get_historical_record_count(feature_group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=\"ml.m5.4xlarge\"\n",
    ")\n",
    "\n",
    "input_flow= ParameterString(\n",
    "    name='InputFlow',\n",
    "    default_value= flow_uri\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import Processor\n",
    "\n",
    "container_id = pipeline_utils.get_container(region)\n",
    "\n",
    "container_uri=f\"{container_id}.dkr.ecr.{region}.amazonaws.com/sagemaker-data-wrangler-container:1.x\"\n",
    "\n",
    "processor = Processor(\n",
    "    role=iam_role,\n",
    "    image_uri=container_uri,\n",
    "    instance_count=processing_instance_count,\n",
    "    instance_type=processing_instance_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import FeatureStoreOutput\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "    \n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"DailyJobDataETL\", # DailyFlightDataETL\n",
    "    processor=processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(input_name='flow', \n",
    "                        destination='/opt/ml/processing/flow',\n",
    "                        source=input_flow,\n",
    "                        s3_data_type= 'S3Prefix',\n",
    "                        s3_input_mode= 'File'\n",
    "                       )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=output_name,\n",
    "            app_managed=True, \n",
    "            feature_store_output=FeatureStoreOutput(feature_group_name=feature_group_name))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "# pipeline_name=f\"daily-job-ETL-pipeline-{time.strftime('%d-%H-%M-%S', time.gmtime())}\"\n",
    "pipeline_name=f\"daily-featurestore-preprocessing-ETL\" ## Replaced 3/6/2022 to determine if any other modifications are needed to convert pipeline name to not include date timestamp\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type, \n",
    "        processing_instance_count,\n",
    "        input_flow\n",
    "    ],\n",
    "    steps=[step_process],\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:769265885190:pipeline/daily-job-etl-pipeline-02-18-04-03',\n",
       " 'ResponseMetadata': {'RequestId': '92220fb2-122f-4f70-b552-ea12d927daea',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '92220fb2-122f-4f70-b552-ea12d927daea',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '102',\n",
       "   'date': 'Wed, 02 Mar 2022 18:04:05 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(iam_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating an IAM role for AWS Lambda function ...\n",
      "SUCCESS: Successfully created IAM role for AWS Lambda function!\n",
      "Adding permissions to AWS Lambda function's IAM role ...\n",
      "SUCCESS: Successfully added permissions AWS Lambda function's IAM role!\n",
      "Waiting for 30 seconds for the newly created role to be active.\n",
      "30 seconds are up; proceeding with rest of the execution.\n"
     ]
    }
   ],
   "source": [
    "prefix='daily_data'\n",
    "\n",
    "role_name = f\"sm-lambda-role-{time.strftime('%d-%H-%M-%S', time.gmtime())}\"\n",
    "fcn_name = f\"sm-lambda-fcn-{time.strftime('%d-%H-%M-%S', time.gmtime())}\"\n",
    "\n",
    "account_num = boto3.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "#Create IAM role for the Lambda function\n",
    "lambda_role = pipeline_utils.create_role(role_name)\n",
    "\n",
    "#Wait for the role to be activated\n",
    "print('Waiting for 30 seconds for the newly created role to be active.')\n",
    "time.sleep(30)\n",
    "print('30 seconds are up; proceeding with rest of the execution.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering variables ...\n",
      "Creating code for AWS Lambda function ...\n",
      "SUCCESS: Successfully created code for AWS Lambda function!\n"
     ]
    }
   ],
   "source": [
    "#Create code for AWS Lambda function\n",
    "lambda_code = pipeline_utils.create_lambda_fcn(flow_uri, pipeline_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    import json\n",
      "    import boto3\n",
      "\n",
      "    s3 = boto3.resource('s3')\n",
      "    sm = boto3.client('sagemaker')\n",
      "\n",
      "    def lambda_handler(event, context):\n",
      "\n",
      "        #Check version of Boto3 - It must be at least 1.16.55\n",
      "        print(f\"The version of Boto3 is {boto3.__version__}\")\n",
      "\n",
      "        #Get location for where the new data (csv) file was uploaded\n",
      "        data_bucket = event['Records'][0]['s3']['bucket']['name']\n",
      "        data_key = event['Records'][0]['s3']['object']['key']\n",
      "        print(f\"A new file named {data_key} was just uploaded to Amazon S3 in {data_bucket}\")\n",
      "\n",
      "        #Update values for where Data Wrangler .flow is saved\n",
      "        flow_bucket = 'sagemaker-us-east-1-769265885190'\n",
      "        flow_key = 'data_wrangler_flows/flow-01-01-10-10-33a59ad7.flow'\n",
      "        pipeline_name = 'daily-job-ETL-pipeline-02-18-04-03'\n",
      "        execution_display = f\"{data_key.split('/')[-1].replace('_','').replace('.csv','')}\"\n",
      "\n",
      "\n",
      "        #Get .flow file from Amazon S3\n",
      "        get_object = s3.Object(flow_bucket,flow_key)\n",
      "        get_flow = get_object.get()\n",
      "\n",
      "        #Read, update and save the .flow file\n",
      "        flow_content = json.loads(get_flow['Body'].read())\n",
      "        flow_content['nodes'][0]['parameters']['dataset_definition']['name'] = data_key.split('/')[-1]\n",
      "        flow_content['nodes'][0]['parameters']['dataset_definition']['s3ExecutionContext']['s3Uri'] = f\"s3://{data_bucket}/{data_key}\"\n",
      "        new_flow_key = flow_key.replace('.flow', '-' + data_key.split('/')[-1].replace('.csv','') + '.flow')\n",
      "        new_flow_uri = f\"s3://{flow_bucket}/{new_flow_key}\"\n",
      "        put_object = s3.Object(flow_bucket,new_flow_key)\n",
      "        put_flow = put_object.put(Body=json.dumps(flow_content))\n",
      "\n",
      "\n",
      "        #Start the pipeline execution\n",
      "        start_pipeline = sm.start_pipeline_execution(\n",
      "                        PipelineName=pipeline_name,\n",
      "                        PipelineExecutionDisplayName=f\"{data_key.split('/')[-1].replace('_','').replace('.csv','')}\",\n",
      "                        PipelineParameters=[\n",
      "                            {\n",
      "                                'Name': 'InputFlow',\n",
      "                                'Value': new_flow_uri\n",
      "                            },\n",
      "                        ],\n",
      "                        PipelineExecutionDescription=data_key\n",
      "                        )\n",
      "        print(start_pipeline)\n",
      "\n",
      "\n",
      "        return('SageMaker Pipeline has been successfully executed')\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(lambda_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating AWS Lambda function ...\n",
      "SUCCESS: Successfully created AWS Lambda function!\n"
     ]
    }
   ],
   "source": [
    "#Zip AWS Lambda function code\n",
    "#Write code to a .py file\n",
    "with open('lambda_function.py', 'w') as f:\n",
    "    f.write(inspect.cleandoc(lambda_code))\n",
    "#Compress file into a zip\n",
    "with ZipFile('function.zip','w') as z:\n",
    "    z.write('lambda_function.py')\n",
    "#Use zipped code as AWS Lambda function code\n",
    "with open('lambda_function.py', 'w') as f:\n",
    "    f.write(lambda_code)\n",
    "\n",
    "#Create AWS Lambda function\n",
    "with open('function.zip', 'rb') as f:\n",
    "    fcn_code = f.read()   \n",
    "lambda_arn = pipeline_utils.create_lambda(fcn_name, fcn_code, lambda_role['arn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sm-lambda-fcn-02-18-07-33\n"
     ]
    }
   ],
   "source": [
    "print(fcn_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-769265885190\n"
     ]
    }
   ],
   "source": [
    "print(default_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily_data\n"
     ]
    }
   ],
   "source": [
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769265885190\n"
     ]
    }
   ],
   "source": [
    "print(account_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:lambda:us-east-1:769265885190:function:sm-lambda-fcn-02-18-07-33\n"
     ]
    }
   ],
   "source": [
    "print(lambda_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding permissions to Amazon S3 ...\n",
      "SUCCESS: Successfully added permissions to Amazon S3!\n",
      "Initialising Amazon S3 Bucket client ...\n",
      "SUCCESS: Successfully initilised Amazon S3 Bucket client!\n",
      "Setting up notifications on Amazon S3 Bucket\n",
      "SUCCESS: Successfully added notifications to Amazon S3 Bucket!\n"
     ]
    }
   ],
   "source": [
    "#Add permission for Amazon S3 to trigger AWS Lambda and set up trigger\n",
    "pipeline_utils.create_s3_trigger(fcn_name, default_bucket, prefix, account_num, lambda_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.s3.S3Uploader.upload(\"../sagemaker-banking-classification-p-apjvzlbx2a9o-modelbuild/bank-additional-full.csv\", f\"s3://{default_bucket}/{prefix}\")\n",
    "#wait for file to finish uploading \n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:769265885190:pipeline/daily-job-etl-pipeline-02-18-04-03',\n",
       " 'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:769265885190:pipeline/daily-job-etl-pipeline-02-18-04-03/execution/9nz01atwzybk',\n",
       " 'PipelineExecutionDisplayName': 'bank-additional-full',\n",
       " 'PipelineExecutionStatus': 'Executing',\n",
       " 'PipelineExecutionDescription': 'daily_data/bank-additional-full.csv',\n",
       " 'CreationTime': datetime.datetime(2022, 3, 2, 18, 18, 54, 926000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2022, 3, 2, 18, 18, 54, 926000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {},\n",
       " 'LastModifiedBy': {},\n",
       " 'ResponseMetadata': {'RequestId': '776bea64-68f4-4e6f-90aa-ce101bb879e3',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '776bea64-68f4-4e6f-90aa-ce101bb879e3',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '499',\n",
       "   'date': 'Wed, 02 Mar 2022 18:19:15 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check pipeline execution \n",
    "latest_execution = sm_client.list_pipeline_executions(PipelineName=pipeline_name).get('PipelineExecutionSummaries')[0].get('PipelineExecutionArn')\n",
    "sm_client.describe_pipeline_execution(PipelineExecutionArn=latest_execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_id='services'\n",
    "sample_record = sess.boto_session.client('sagemaker-featurestore-runtime', region_name=region).get_record(FeatureGroupName=feature_group_name, RecordIdentifierValueAsString=str(record_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '1811159c-94b2-452a-9a3a-93be71422d83',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '1811159c-94b2-452a-9a3a-93be71422d83',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '193',\n",
       "   'date': 'Wed, 02 Mar 2022 18:24:42 GMT'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Record': [{'FeatureName': 'RECORD_ID', 'ValueAsString': 'services'},\n",
       "  {'FeatureName': 'y', 'ValueAsString': '0.08138070042831948'},\n",
       "  {'FeatureName': 'EVENT_TIME', 'ValueAsString': '2022-03-02T18:24:11Z'}]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
